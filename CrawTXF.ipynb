{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Craw TXF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io, datetime, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def craw(date):\n",
    "    # 將傳進來的date解析成 [year, month, day]\n",
    "    lst = [date.year, date.month, date.day]\n",
    "    # 直接將list丟進url中, 獲取對應的日期網址\n",
    "    url = 'https://www.taifex.com.tw/DailyDownload/DailyDownloadCSV/Daily_{:04d}_{:02d}_{:02d}.zip'.format(*lst)\n",
    "    \n",
    "    # 迴圈開始爬蟲程式\n",
    "    while True:\n",
    "        # sleep起手\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            # 使用get方法, 速度比post快\n",
    "            response = requests.get(url)\n",
    "            # response用content方式獲取, 這是二進位輸出, 目的是用io接出使用zipfile來讀取\n",
    "            with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "                # 使用namelist拿出每一個zipfile中的檔案名字\n",
    "                for file in z.namelist():\n",
    "                    # 將檔案解開至指定目錄\n",
    "                    z.extract(file, 'data/txf_daily_csv')\n",
    "            print(date, 'successed.')\n",
    "            break\n",
    "        except ConnectionRefusedError as e:\n",
    "            print(date, e)\n",
    "            continue\n",
    "        # 記得捕捉非zip格式, 在這邊這代表當天沒資料\n",
    "        except zipfile.BadZipFile as e:\n",
    "            print(date, e)\n",
    "            break\n",
    "    # 回傳, 避免沒有捕捉到的錯誤, 有回傳我們還可以使用response來補救, 不用全部重跑\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 由於期交所只保留30天資料, 因此range就不要設超過30了\n",
    "    # 這邊定義30天的list, 準備去抓對應的資料\n",
    "    days = [datetime.date.today() - datetime.timedelta(i) for i in range(30)]\n",
    "    # 跑回圈去抓資料\n",
    "    responses = [craw(date) for date in days]\n",
    "    # 結束\n",
    "    print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-17 successed.\n",
      "2018-09-16 File is not a zip file\n",
      "2018-09-15 File is not a zip file\n",
      "2018-09-14 successed.\n",
      "2018-09-13 successed.\n",
      "2018-09-12 successed.\n",
      "2018-09-11 successed.\n",
      "2018-09-10 successed.\n",
      "2018-09-09 File is not a zip file\n",
      "2018-09-08 File is not a zip file\n",
      "2018-09-07 successed.\n",
      "2018-09-06 successed.\n",
      "2018-09-05 successed.\n",
      "2018-09-04 successed.\n",
      "2018-09-03 successed.\n",
      "2018-09-02 File is not a zip file\n",
      "2018-09-01 File is not a zip file\n",
      "2018-08-31 successed.\n",
      "2018-08-30 successed.\n",
      "2018-08-29 successed.\n",
      "2018-08-28 successed.\n",
      "2018-08-27 successed.\n",
      "2018-08-26 File is not a zip file\n",
      "2018-08-25 File is not a zip file\n",
      "2018-08-24 successed.\n",
      "2018-08-23 successed.\n",
      "2018-08-22 successed.\n",
      "2018-08-21 successed.\n",
      "2018-08-20 successed.\n",
      "2018-08-19 File is not a zip file\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到這邊, 這個爬蟲程式就可以抓對應的zip檔, 並將之解壓縮到指定目錄.  \n",
    "為了能夠在命令列視窗中執行, 我們把這些程式碼移植到`CrawTXF.py`中.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用內建的`argparse`套件, 這能夠幫助我們傳入參數.  \n",
    "在程式一開始新增如下的程式碼:  \n",
    "`parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-n', dest='n_days')\n",
    "args = parser.parse_args()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "透過上面的程式碼就可以在命令列視窗中帶入`-n`的參數.  \n",
    "接著修改`main()`如下:  \n",
    "`def main():\n",
    "    if args.n_days == None:\n",
    "        n_days = 30\n",
    "    else:\n",
    "        n_days = int(args.n_days)\n",
    "    days = [datetime.date.today() - datetime.timedelta(i) for i in range(n_days)]\n",
    "    [craw(date) for date in days]\n",
    "    print('Finished.')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著我們就可以在命令列中輸入以下命令來執行程式:  \n",
    "`python3 CrawTXF.py -n 1`  \n",
    "如果成功執行以後就可以達到排程每天抓當天資料, 並且有問題也可以手動執行回補30天的效果了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
